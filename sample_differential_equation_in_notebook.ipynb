Now we'll take our data and process it through the sigmoid function:
$$h_\theta(x) = g(\theta^Tx): g(z) = \frac{1}{1+e^{-z}}$$

The logistic regression cost function including regularization (last term) is:

$$J(\theta) = \frac{1}{m}\sum\limits_{i=1}^{m} [-y^{(i)}log(h_\theta(x^{(i)})-(1 - y^{(i)})log(1 - h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta^2_j$$

The gradient of the cost including regularization is:

$$\frac{\partial{J(\theta)}}{\partial{\theta_j}} = \frac{1}{m}\sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}     \qquad \qquad \qquad   for \,\, j=0 $$

$$\frac{\partial{J(\theta)}}{\partial{\theta_j}} = (\frac{1}{m}\sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j    \qquad    for \,\, j \geq 1 $$
